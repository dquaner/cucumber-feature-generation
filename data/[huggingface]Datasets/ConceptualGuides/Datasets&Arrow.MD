# Datasets ðŸ¤ Arrow

## Arrow æ˜¯ä»€ä¹ˆï¼Ÿ

[Arrow](https://arrow.apache.org/) èƒ½å¤Ÿå¿«é€Ÿå¤„ç†å’Œç§»åŠ¨å¤§é‡æ•°æ®ã€‚å®ƒæ˜¯ä¸€ç§ç‰¹å®šçš„æ•°æ®æ ¼å¼ï¼Œå°†æ•°æ®å­˜å‚¨åœ¨ä¸€ç§åˆ—å¼å†…å­˜å¸ƒå±€ä¸­ã€‚å®ƒæä¾›äº†å‡ ä¸ªæ˜¾è‘—çš„ä¼˜åŠ¿ï¼š

* Arrow çš„æ ‡å‡†æ ¼å¼å…è®¸[é›¶æ‹·è´è¯»å–](https://en.wikipedia.org/wiki/Zero-copy)ï¼Œè¿™å‡ ä¹Žæ¶ˆé™¤äº†æ‰€æœ‰åºåˆ—åŒ–å¼€é”€ã€‚
* Arrow æ˜¯è¯­è¨€æ— å…³çš„ï¼Œæ‰€ä»¥å®ƒæ”¯æŒä¸åŒçš„ç¼–ç¨‹è¯­è¨€ã€‚
* Arrow æ˜¯é¢å‘åˆ—çš„ï¼Œæ‰€ä»¥å®ƒåœ¨æŸ¥è¯¢å’Œå¤„ç†æ•°æ®ç‰‡æˆ–åˆ—æ—¶é€Ÿåº¦æ›´å¿«ã€‚
* Arrow å…è®¸æ— å¤åˆ¶åˆ‡æ¢åˆ°æ ‡å‡†æœºå™¨å­¦ä¹ å·¥å…·ï¼Œå¦‚ NumPy, Pandas, PyTorch å’ŒT ensorFlowã€‚
* Arrow æ”¯æŒè®¸å¤šåˆ—ç±»åž‹ï¼ŒåŒ…æ‹¬åµŒå¥—çš„åˆ—ç±»åž‹ã€‚

## å†…å­˜æ˜ å°„

ðŸ¤— Datasets ä½¿ç”¨ Arrow ä½œä¸ºå…¶æœ¬åœ°ç¼“å­˜ç³»ç»Ÿã€‚å®ƒå…è®¸æ•°æ®é›†ç”±ç£ç›˜ç¼“å­˜æ”¯æŒï¼Œç£ç›˜ç¼“å­˜æ˜¯å†…å­˜æ˜ å°„çš„ï¼Œç”¨äºŽå¿«é€ŸæŸ¥æ‰¾ã€‚
è¿™ç§æž¶æž„å…è®¸åœ¨è®¾å¤‡å†…å­˜ç›¸å¯¹è¾ƒå°çš„æœºå™¨ä¸Šä½¿ç”¨å¤§åž‹æ•°æ®é›†ã€‚

ä¾‹å¦‚ï¼ŒåŠ è½½å®Œæ•´çš„è‹±æ–‡ç»´åŸºç™¾ç§‘æ•°æ®é›†åªéœ€è¦å‡ MBçš„ RAMï¼š

```python
>>> import os; import psutil; import timeit
>>> from datasets import load_dataset

# Process.memory_info is expressed in bytes, so convert to megabytes 
>>> mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)
>>> wiki = load_dataset("wikipedia", "20220301.en", split="train")
>>> mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)

>>> print(f"RAM memory used: {(mem_after - mem_before)} MB")
RAM memory used: 50 MB
```

è¿™æ˜¯å¯èƒ½çš„ï¼Œå› ä¸º Arrow æ•°æ®å®žé™…ä¸Šæ˜¯ä»Žç£ç›˜æ˜ å°„åˆ°å†…å­˜çš„ï¼Œè€Œä¸æ˜¯åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
å†…å­˜æ˜ å°„å…è®¸è®¿é—®ç£ç›˜ä¸Šçš„æ•°æ®ï¼Œå¹¶åˆ©ç”¨è™šæ‹Ÿå†…å­˜åŠŸèƒ½è¿›è¡Œå¿«é€ŸæŸ¥æ‰¾ã€‚

## æ€§èƒ½

ä½¿ç”¨ Arrow è¿­ä»£å†…å­˜æ˜ å°„æ•°æ®é›†çš„é€Ÿåº¦å¾ˆå¿«ã€‚åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿­ä»£ç»´åŸºç™¾ç§‘æ•°æ® ï¼Œä½ å¯ä»¥èŽ·å¾— 1-3 Gbit/s çš„é€Ÿåº¦:

```python
>>> s = """batch_size = 1000
... for batch in wiki.iter(batch_size):
...     ...
... """

>>> time = timeit.timeit(stmt=s, number=1, globals=globals())
>>> print(f"Time to iterate over the {wiki.dataset_size >> 30} GB dataset: {time:.1f} sec, "
...       f"ie. {float(wiki.dataset_size >> 27)/time:.1f} Gb/s")
Time to iterate over the 18 GB dataset: 31.8 sec, ie. 4.8 Gb/s
```